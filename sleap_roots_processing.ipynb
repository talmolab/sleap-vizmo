{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP-Roots Processing with sleap-vizmo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load SLEAP files using sleap-io\n",
    "2. Split multi-video labels into individual files\n",
    "3. Save files with correct naming for sleap-roots Series\n",
    "4. Process with MultipleDicotPipeline to get traits for multiple plants\n",
    "5. Generate a CSV with all plant associations and traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sleap_io as sio\n",
    "import sleap_roots as sr\n",
    "from sleap_roots.trait_pipelines import MultipleDicotPipeline\n",
    "from sleap_vizmo.roots_utils import (\n",
    "    split_labels_by_video,\n",
    "    save_individual_video_labels,\n",
    "    validate_series_compatibility,\n",
    "    create_series_name_from_video\n",
    ")\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test SLEAP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SLEAP files...\n",
      "Lateral labels: 23 frames, 23 videos\n",
      "Primary labels: 23 frames, 23 videos\n"
     ]
    }
   ],
   "source": [
    "# Define paths to test data\n",
    "test_data_dir = Path(\"tests/data\")\n",
    "lateral_file = test_data_dir / \"lateral_root_MK22_Day14_labels.v002.slp\"\n",
    "primary_file = test_data_dir / \"primary_root_MK22_Day14_labels.v003.slp\"\n",
    "\n",
    "# Load the SLEAP files\n",
    "print(\"Loading SLEAP files...\")\n",
    "lateral_labels = sio.load_slp(lateral_file)\n",
    "primary_labels = sio.load_slp(primary_file)\n",
    "\n",
    "print(f\"Lateral labels: {len(lateral_labels)} frames, {len(lateral_labels.videos)} videos\")\n",
    "print(f\"Primary labels: {len(primary_labels)} frames, {len(primary_labels.videos)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Series Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lateral labels compatibility:\n",
      "  Compatible: True\n",
      "  Warnings: ['No tracks found - Series may expect tracked data', 'Labels contain 23 videos. Series typically expects one video per file. Consider using split_labels_by_video().']\n",
      "\n",
      "Primary labels compatibility:\n",
      "  Compatible: True\n",
      "  Warnings: ['No tracks found - Series may expect tracked data', 'Labels contain 23 videos. Series typically expects one video per file. Consider using split_labels_by_video().']\n"
     ]
    }
   ],
   "source": [
    "# Check if labels are compatible with Series requirements\n",
    "lateral_compat = validate_series_compatibility(lateral_labels)\n",
    "primary_compat = validate_series_compatibility(primary_labels)\n",
    "\n",
    "print(\"Lateral labels compatibility:\")\n",
    "print(f\"  Compatible: {lateral_compat['is_compatible']}\")\n",
    "if lateral_compat['warnings']:\n",
    "    print(f\"  Warnings: {lateral_compat['warnings']}\")\n",
    "if lateral_compat['errors']:\n",
    "    print(f\"  Errors: {lateral_compat['errors']}\")\n",
    "\n",
    "print(\"\\nPrimary labels compatibility:\")\n",
    "print(f\"  Compatible: {primary_compat['is_compatible']}\")\n",
    "if primary_compat['warnings']:\n",
    "    print(f\"  Warnings: {primary_compat['warnings']}\")\n",
    "if primary_compat['errors']:\n",
    "    print(f\"  Errors: {primary_compat['errors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Labels by Video and Save with Proper Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: output/sleap_roots_processing_20250804_075350_702843\n",
      "\n",
      "Lateral labels split into 23 video(s)\n",
      "Primary labels split into 23 video(s)\n"
     ]
    }
   ],
   "source": [
    "# Create timestamped output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "output_dir = Path(\"output\") / f\"sleap_roots_processing_{timestamp}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Split labels by video if needed\n",
    "lateral_split = split_labels_by_video(lateral_labels)\n",
    "primary_split = split_labels_by_video(primary_labels)\n",
    "\n",
    "print(f\"\\nLateral labels split into {len(lateral_split)} video(s)\")\n",
    "print(f\"Primary labels split into {len(primary_split)} video(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving lateral root files...\n",
      "  Saved: F_Ac_set1_day14_20250527_102755_001.lateral.slp\n",
      "  Saved: F_Cp_set1_day14_20250527_102755_002.lateral.slp\n",
      "  Saved: F_De_set1_day14_20250527_102755_003.lateral.slp\n",
      "  Saved: F_DhA_set1_day14_20250527_102755_004.lateral.slp\n",
      "  Saved: F_DhD_set1_day14_20250527_102755_005.lateral.slp\n",
      "  Saved: F_Fo_set1_day14_20250527_102755_006.lateral.slp\n",
      "  Saved: F_Gr_set1_day14_20250527_102955_007.lateral.slp\n",
      "  Saved: no_peptide1_set1_day14_20250527_102955_010.lateral.slp\n",
      "  Saved: no_peptide2_set1_day14_20250527_102955_011.lateral.slp\n",
      "  Saved: OG_Ac_set2_day14_20250527_103422_014.lateral.slp\n",
      "  Saved: OG_Cp_set2_day14_20250527_103422_015.lateral.slp\n",
      "  Saved: OG_De_set2_day14_20250527_103422_016.lateral.slp\n",
      "  Saved: OG_DhA_set2_day14_20250527_103422_017.lateral.slp\n",
      "  Saved: OG_DhB_set2_day14_20250527_103422_018.lateral.slp\n",
      "  Saved: OG_DhD_set2_day14_20250527_103618_019.lateral.slp\n",
      "  Saved: OG_Fo_set1_day14_20250527_102955_012.lateral.slp\n",
      "  Saved: OG_Gr_set2_day14_20250527_103618_020.lateral.slp\n",
      "  Saved: OG_Mt_set2_day14_20250527_103618_021.lateral.slp\n",
      "  Saved: OG_Ri1_set1_day14_20250527_102955_008.lateral.slp\n",
      "  Saved: OG_Ri2_set1_day14_20250527_102955_009.lateral.slp\n",
      "  Saved: S_Ri_set2_day14_20250527_103422_013.lateral.slp\n",
      "  Saved: F_Ri_set3_day14_20250527_103956_025.lateral.slp\n",
      "  Saved: OG_RiA4_set3_day14_20250527_103956_027.lateral.slp\n",
      "\n",
      "Saving primary root files...\n",
      "  Saved: F_Ac_set1_day14_20250527_102755_001.primary.slp\n",
      "  Saved: F_Cp_set1_day14_20250527_102755_002.primary.slp\n",
      "  Saved: F_De_set1_day14_20250527_102755_003.primary.slp\n",
      "  Saved: F_DhA_set1_day14_20250527_102755_004.primary.slp\n",
      "  Saved: F_DhD_set1_day14_20250527_102755_005.primary.slp\n",
      "  Saved: F_Fo_set1_day14_20250527_102755_006.primary.slp\n",
      "  Saved: F_Gr_set1_day14_20250527_102955_007.primary.slp\n",
      "  Saved: no_peptide1_set1_day14_20250527_102955_010.primary.slp\n",
      "  Saved: no_peptide2_set1_day14_20250527_102955_011.primary.slp\n",
      "  Saved: OG_Ac_set2_day14_20250527_103422_014.primary.slp\n",
      "  Saved: OG_Cp_set2_day14_20250527_103422_015.primary.slp\n",
      "  Saved: OG_De_set2_day14_20250527_103422_016.primary.slp\n",
      "  Saved: OG_DhA_set2_day14_20250527_103422_017.primary.slp\n",
      "  Saved: OG_DhB_set2_day14_20250527_103422_018.primary.slp\n",
      "  Saved: OG_DhD_set2_day14_20250527_103618_019.primary.slp\n",
      "  Saved: OG_Fo_set1_day14_20250527_102955_012.primary.slp\n",
      "  Saved: OG_Gr_set2_day14_20250527_103618_020.primary.slp\n",
      "  Saved: OG_Mt_set2_day14_20250527_103618_021.primary.slp\n",
      "  Saved: OG_Ri1_set1_day14_20250527_102955_008.primary.slp\n",
      "  Saved: OG_Ri2_set1_day14_20250527_102955_009.primary.slp\n",
      "  Saved: S_Ri_set2_day14_20250527_103422_013.primary.slp\n",
      "  Saved: F_Ri_set3_day14_20250527_103956_025.primary.slp\n",
      "  Saved: OG_RiA4_set3_day14_20250527_103956_027.primary.slp\n",
      "\n",
      "Total series to process: 23\n"
     ]
    }
   ],
   "source": [
    "# Save individual video labels with proper naming for Series.load\n",
    "# The naming convention should make it clear which are lateral vs primary\n",
    "\n",
    "series_data = {}  # Will store series names and their file paths\n",
    "\n",
    "# Process lateral roots\n",
    "print(\"\\nSaving lateral root files...\")\n",
    "for video_name, labels in lateral_split.items():\n",
    "    series_name = create_series_name_from_video(video_name)\n",
    "    if series_name not in series_data:\n",
    "        series_data[series_name] = {}\n",
    "    \n",
    "    # Save with .lateral suffix to identify root type\n",
    "    output_path = output_dir / f\"{series_name}.lateral.slp\"\n",
    "    labels.save(str(output_path))\n",
    "    series_data[series_name]['lateral_path'] = str(output_path)\n",
    "    print(f\"  Saved: {output_path.name}\")\n",
    "\n",
    "# Process primary roots\n",
    "print(\"\\nSaving primary root files...\")\n",
    "for video_name, labels in primary_split.items():\n",
    "    series_name = create_series_name_from_video(video_name)\n",
    "    if series_name not in series_data:\n",
    "        series_data[series_name] = {}\n",
    "    \n",
    "    # Save with .primary suffix to identify root type\n",
    "    output_path = output_dir / f\"{series_name}.primary.slp\"\n",
    "    labels.save(str(output_path))\n",
    "    series_data[series_name]['primary_path'] = str(output_path)\n",
    "    print(f\"  Saved: {output_path.name}\")\n",
    "\n",
    "print(f\"\\nTotal series to process: {len(series_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Series and Process with MultipleDicotPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all slp files in the folder\nall_slps = sr.find_all_slp_paths(output_dir)\n\n# Load the cylinder series using slp paths\nall_series = sr.load_series_from_slps(slp_paths=all_slps, h5s=False)\nprint(f\"Loaded {len(all_series)} series\")\nall_series"
  },
  {
   "cell_type": "markdown",
   "source": "## Create Expected Count CSV for MultipleDicotPipeline\n\nThe MultipleDicotPipeline expects a CSV with the number of plants per cylinder. We'll count the number of instances in each primary root file to create this.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create expected count CSV by counting instances in primary root files\nexpected_counts = []\n\nfor series in all_series:\n    # Get the series name which will be our plant_qr_code\n    plant_qr_code = series.series_name\n    \n    # Count the number of instances (plants) in the primary root file\n    # The primary labels contain the instances we need to count\n    if hasattr(series, 'primary_labels') and series.primary_labels is not None:\n        # Count unique instances across all frames\n        all_instances = set()\n        for lf in series.primary_labels:\n            for instance in lf.instances:\n                all_instances.add(instance)\n        \n        num_plants = len(all_instances)\n    else:\n        # If no primary labels, default to 0\n        num_plants = 0\n    \n    # Get the paths for documentation\n    primary_path = \"\"\n    lateral_path = \"\"\n    \n    # Find the corresponding paths from our series_data\n    if plant_qr_code in series_data:\n        primary_path = series_data[plant_qr_code].get('primary_path', '')\n        lateral_path = series_data[plant_qr_code].get('lateral_path', '')\n    \n    # Extract genotype and replicate from the series name\n    # Parse patterns like \"F_Ac_set1_day14_20250527_102755_001\"\n    parts = plant_qr_code.split('_')\n    genotype = \"_\".join(parts[:2]) if len(parts) > 1 else plant_qr_code\n    \n    # Try to extract replicate number from \"set\" part\n    replicate = 1  # default\n    for part in parts:\n        if part.startswith('set'):\n            try:\n                replicate = int(part.replace('set', ''))\n            except:\n                pass\n    \n    # Create row for expected count CSV\n    row = {\n        'plant_qr_code': plant_qr_code,\n        'genotype': genotype,\n        'replicate': replicate,\n        'path': primary_path,  # Using primary path as the main path\n        'qc_cylinder': 0,  # Default value\n        'qc_code': None,  # Will be NaN in CSV\n        'number_of_plants_cylinder': num_plants,\n        'primary_root_proofread': primary_path,\n        'lateral_root_proofread': lateral_path if lateral_path else None,\n    }\n    \n    expected_counts.append(row)\n    print(f\"{plant_qr_code}: {num_plants} plants detected\")\n\n# Create DataFrame\nexpected_count_df = pd.DataFrame(expected_counts)\n\n# Add empty columns to match the expected format\nfor col in ['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Instructions']:\n    expected_count_df[col] = None\n\n# Save the expected count CSV\nexpected_count_path = output_dir / \"expected_plant_counts.csv\"\nexpected_count_df.to_csv(expected_count_path, index=False)\n\nprint(f\"\\n✅ Expected count CSV saved to: {expected_count_path}\")\nprint(f\"Total series: {len(expected_count_df)}\")\nprint(f\"Total plants across all series: {expected_count_df['number_of_plants_cylinder'].sum()}\")\n\n# Display the dataframe\ndisplay(expected_count_df[['plant_qr_code', 'genotype', 'replicate', 'number_of_plants_cylinder']].head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display the expected count CSV for verification\nprint(\"Expected Count CSV Preview:\")\nprint(f\"Shape: {expected_count_df.shape}\")\nprint(\"\\nFirst 10 rows:\")\ndisplay(expected_count_df.head(10))\n\n# Show summary statistics\nprint(f\"\\nPlant count distribution:\")\nprint(expected_count_df['number_of_plants_cylinder'].value_counts().sort_index())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing series: F_Ac_set1_day14_20250527_102755_001\n",
      "  Primary: F_Ac_set1_day14_20250527_102755_001.primary.slp\n",
      "  Lateral: F_Ac_set1_day14_20250527_102755_001.lateral.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_Cp_set1_day14_20250527_102755_002\n",
      "  Primary: F_Cp_set1_day14_20250527_102755_002.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_De_set1_day14_20250527_102755_003\n",
      "  Primary: F_De_set1_day14_20250527_102755_003.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_DhA_set1_day14_20250527_102755_004\n",
      "  Primary: F_DhA_set1_day14_20250527_102755_004.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_DhD_set1_day14_20250527_102755_005\n",
      "  Primary: F_DhD_set1_day14_20250527_102755_005.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_Fo_set1_day14_20250527_102755_006\n",
      "  Primary: F_Fo_set1_day14_20250527_102755_006.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_Gr_set1_day14_20250527_102955_007\n",
      "  Primary: F_Gr_set1_day14_20250527_102955_007.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: no_peptide1_set1_day14_20250527_102955_010\n",
      "  Primary: no_peptide1_set1_day14_20250527_102955_010.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: no_peptide2_set1_day14_20250527_102955_011\n",
      "  Primary: no_peptide2_set1_day14_20250527_102955_011.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Ac_set2_day14_20250527_103422_014\n",
      "  Primary: OG_Ac_set2_day14_20250527_103422_014.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Cp_set2_day14_20250527_103422_015\n",
      "  Primary: OG_Cp_set2_day14_20250527_103422_015.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_De_set2_day14_20250527_103422_016\n",
      "  Primary: OG_De_set2_day14_20250527_103422_016.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_DhA_set2_day14_20250527_103422_017\n",
      "  Primary: OG_DhA_set2_day14_20250527_103422_017.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_DhB_set2_day14_20250527_103422_018\n",
      "  Primary: OG_DhB_set2_day14_20250527_103422_018.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_DhD_set2_day14_20250527_103618_019\n",
      "  Primary: OG_DhD_set2_day14_20250527_103618_019.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Fo_set1_day14_20250527_102955_012\n",
      "  Primary: OG_Fo_set1_day14_20250527_102955_012.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Gr_set2_day14_20250527_103618_020\n",
      "  Primary: OG_Gr_set2_day14_20250527_103618_020.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Mt_set2_day14_20250527_103618_021\n",
      "  Primary: OG_Mt_set2_day14_20250527_103618_021.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Ri1_set1_day14_20250527_102955_008\n",
      "  Primary: OG_Ri1_set1_day14_20250527_102955_008.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_Ri2_set1_day14_20250527_102955_009\n",
      "  Primary: OG_Ri2_set1_day14_20250527_102955_009.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: S_Ri_set2_day14_20250527_103422_013\n",
      "  Primary: S_Ri_set2_day14_20250527_103422_013.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: F_Ri_set3_day14_20250527_103956_025\n",
      "  Primary: F_Ri_set3_day14_20250527_103956_025.primary.slp\n",
      "  ✓ Series loaded successfully\n",
      "\n",
      "Processing series: OG_RiA4_set3_day14_20250527_103956_027\n",
      "  Primary: OG_RiA4_set3_day14_20250527_103956_027.primary.slp\n",
      "  ✓ Series loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load each series and process with MultipleDicotPipeline\n",
    "all_series = []\n",
    "all_traits = []\n",
    "\n",
    "for series_name, paths in series_data.items():\n",
    "    print(f\"\\nProcessing series: {series_name}\")\n",
    "    \n",
    "    # Prepare kwargs for Series.load\n",
    "    load_kwargs = {'series_name': series_name}\n",
    "    \n",
    "    if 'primary_path' in paths:\n",
    "        load_kwargs['primary_path'] = paths['primary_path']\n",
    "        print(f\"  Primary: {Path(paths['primary_path']).name}\")\n",
    "    \n",
    "    if 'lateral_path' in paths:\n",
    "        load_kwargs['lateral_path'] = paths['lateral_path']\n",
    "        print(f\"  Lateral: {Path(paths['lateral_path']).name}\")\n",
    "    \n",
    "    # Load the series\n",
    "    try:\n",
    "        series = sr.Series.load(**load_kwargs)\n",
    "        all_series.append(series)\n",
    "        print(f\"  ✓ Series loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error loading series: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize MultipleDicotPipeline\npipeline = MultipleDicotPipeline()\nprint(f\"\\nUsing pipeline: {pipeline.__class__.__name__}\")\n\n# The MultipleDicotPipeline expects an expected count CSV\n# Let's use the one we just created\nprint(f\"Using expected count CSV: {expected_count_path}\")\n\n# Process all series together with the expected count CSV\ntry:\n    # Compute traits for multiple plants across all series\n    traits = pipeline.compute_multiple_dicots_traits(\n        all_series,\n        write_csv=True,\n        csv_suffix=\"_all_plants_traits.csv\",\n        output_dir=str(output_dir),\n        expected_count_csv_path=str(expected_count_path)  # Pass the expected count CSV\n    )\n    \n    # Handle the output\n    if isinstance(traits, pd.DataFrame):\n        all_traits_df = traits\n        print(f\"✓ Computed traits for {len(all_traits_df)} plants\")\n    else:\n        # Handle case where traits might be a list of dictionaries\n        all_traits_df = pd.DataFrame(traits)\n        print(f\"✓ Computed traits for {len(all_traits_df)} plants\")\n        \nexcept Exception as e:\n    print(f\"✗ Error computing traits: {e}\")\n    import traceback\n    traceback.print_exc()\n    all_traits_df = pd.DataFrame()  # Empty dataframe on error"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine All Traits into Final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if we have traits data\nif 'all_traits_df' in locals() and len(all_traits_df) > 0:\n    # Save the final CSV with all plants and their traits\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n    final_csv_path = output_dir / f\"all_plants_traits_{timestamp}.csv\"\n    all_traits_df.to_csv(final_csv_path, index=False)\n    \n    print(f\"\\n✅ Final CSV saved: {final_csv_path}\")\n    print(f\"Total plants processed: {len(all_traits_df)}\")\n    print(f\"\\nColumns in final CSV:\")\n    for col in all_traits_df.columns:\n        print(f\"  - {col}\")\n    \n    # Display first few rows\n    print(\"\\nFirst 5 rows of the final DataFrame:\")\n    display(all_traits_df.head())\nelse:\n    print(\"\\n⚠️ No traits were computed successfully\")\n    print(\"Check that:\")\n    print(\"  1. The expected count CSV was created correctly\")\n    print(\"  2. The series have valid primary root instances\")\n    print(\"  3. The MultipleDicotPipeline can process the data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a summary of the processing\nsummary = {\n    \"timestamp\": timestamp,\n    \"output_directory\": str(output_dir),\n    \"input_files\": {\n        \"lateral\": str(lateral_file),\n        \"primary\": str(primary_file)\n    },\n    \"series_processed\": len(all_series),\n    \"total_plants\": len(all_traits_df) if 'all_traits_df' in locals() and len(all_traits_df) > 0 else 0,\n    \"pipeline_used\": \"MultipleDicotPipeline\",\n    \"expected_count_csv\": str(expected_count_path),\n    \"expected_total_plants\": expected_count_df['number_of_plants_cylinder'].sum(),\n    \"trait_columns\": list(all_traits_df.columns) if 'all_traits_df' in locals() and len(all_traits_df) > 0 else []\n}\n\n# Save summary as JSON\nsummary_path = output_dir / \"processing_summary.json\"\nwith open(summary_path, 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(f\"\\n📊 Processing Summary:\")\nprint(f\"  - Series processed: {summary['series_processed']}\")\nprint(f\"  - Expected plants: {summary['expected_total_plants']}\")\nprint(f\"  - Actual plants processed: {summary['total_plants']}\")\nprint(f\"  - Output directory: {summary['output_directory']}\")\nprint(f\"  - Summary saved to: {summary_path.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: Visualize trait distributions\nif 'all_traits_df' in locals() and len(all_traits_df) > 0:\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    # Set up the plot style\n    plt.style.use('seaborn-v0_8-whitegrid')\n    \n    # Select numeric columns for visualization\n    numeric_cols = all_traits_df.select_dtypes(include=['float64', 'int64']).columns\n    \n    if len(numeric_cols) > 0:\n        # Create a figure with subplots for first few traits\n        n_traits = min(6, len(numeric_cols))\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        axes = axes.flatten()\n        \n        for i, col in enumerate(numeric_cols[:n_traits]):\n            axes[i].hist(all_traits_df[col].dropna(), bins=20, edgecolor='black', alpha=0.7)\n            axes[i].set_title(col.replace('_', ' ').title())\n            axes[i].set_xlabel('Value')\n            axes[i].set_ylabel('Count')\n        \n        # Hide any unused subplots\n        for i in range(n_traits, 6):\n            axes[i].set_visible(False)\n        \n        plt.suptitle('Distribution of Plant Traits', fontsize=16)\n        plt.tight_layout()\n        \n        # Save the figure\n        fig_path = output_dir / \"trait_distributions.png\"\n        plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        print(f\"\\n📈 Trait distribution plot saved to: {fig_path.name}\")\nelse:\n    print(\"\\n📊 No traits data available for visualization\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap-vizmo-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}